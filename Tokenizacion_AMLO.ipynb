{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c41ed2c-9e57-47fd-bb47-2e2ecf70c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenizado: 1301_-E8FoDOc0Do.txt\n",
      " Tokenizado: 1302_ZT-Ig2EdmeE.txt\n",
      " Tokenizado: 1303_cbq-2Qqt440.txt\n",
      " Tokenizado: 1304_Kq8rfmmDUAk.txt\n",
      " Tokenizado: 1305_C8KQaZB2uZk.txt\n",
      " Tokenizado: 1306_99LX5sXRK-o.txt\n",
      " Tokenizado: 1307_Tks4lVdO3cQ.txt\n",
      " Tokenizado: 1308_ji0vHXs6mkY.txt\n",
      " Tokenizado: 1309_iIw5NUWas30.txt\n",
      " Tokenizado: 1310_wOivNIFTUd4.txt\n",
      " Tokenizado: 1311_JZELcpjRJ-0.txt\n",
      " Tokenizado: 1312_RzpUB34gmBg.txt\n",
      " Tokenizado: 1313_p_j7rPIWA94.txt\n",
      " Tokenizado: 1314_-ugBfd4HRt0.txt\n",
      " Tokenizado: 1315_tdBoWplosrU.txt\n",
      " Tokenizado: 1316_kNrtIu5iwwI.txt\n",
      " Tokenizado: 1317_VfqNBEPkUyM.txt\n",
      " Tokenizado: 1318_6hSjYgA9W6A.txt\n",
      " Tokenizado: 1319_D4ezrsRUnCk.txt\n",
      " Tokenizado: 1320_OWIzaUQeCeo.txt\n",
      " Tokenizado: 1321_znd9eAdY1d8.txt\n",
      " Tokenizado: 1322_Nda0Vw_z14E.txt\n",
      " Tokenizado: 1323_Vbl-pQL4v9M.txt\n",
      " Tokenizado: 1324_p31Vbrik4JQ.txt\n",
      " Tokenizado: 1325_kf8zvxQvI2Q.txt\n",
      " Tokenizado: 1326_EFK4Fg7gJto.txt\n",
      " Tokenizado: 1327_nY7RlwVemDk.txt\n",
      " Tokenizado: 1328_nvbgCu08syE.txt\n",
      " Tokenizado: 1329_OM7Cw92ax3s.txt\n",
      " Tokenizado: 1330_lFhbSavUfGg.txt\n",
      " Tokenizado: 1331_xMm8wlXnKpM.txt\n",
      " Tokenizado: 1332_gPdNjjvgtTU.txt\n",
      " Tokenizado: 1333_Nyh3i6WLmog.txt\n",
      " Tokenizado: 1334_SVGkvx5ezck.txt\n",
      " Tokenizado: 1335_9AX__L-CmGY.txt\n",
      " Tokenizado: 1336_MYjTphCLP1E.txt\n",
      " Tokenizado: 1337_OUhYpOJ62ZA.txt\n",
      " Tokenizado: 1338_vJfR8G9-e_Y.txt\n",
      " Tokenizado: 1339_JfIe3WE5EmE.txt\n",
      " Tokenizado: 1340_BW0otqLxES0.txt\n",
      " Tokenizado: 1341_wkmMzWXsTW4.txt\n",
      " Tokenizado: 1342_jxuHN8CUaxg.txt\n",
      " Tokenizado: 1343_vtkTLKQEh5I.txt\n",
      " Tokenizado: 1344_ky9JIVWELR8.txt\n",
      " Tokenizado: 1345_TBBLuf-ib3o.txt\n",
      " Tokenizado: 1346_mTCUsv519lk.txt\n",
      " Tokenizado: 1347_rprBvY1mDAA.txt\n",
      " Tokenizado: 1348_95yxGjAibLE.txt\n",
      " Tokenizado: 1349_TpHXLuU-ge0.txt\n",
      " Tokenizado: 1350_uC85BJCjD4A.txt\n",
      " Tokenizado: 1351_OdkTrKp7pSc.txt\n",
      " Tokenizado: 1352_X6eLfT8lXic.txt\n",
      " Tokenizado: 1353_g9EHXYnksSI.txt\n",
      " Tokenizado: 1354_meFBP8v_dAY.txt\n",
      " Tokenizado: 1355_jIv4DBPTjkQ.txt\n",
      " Tokenizado: 1356_lrdK6QawuDk.txt\n",
      " Tokenizado: 1357_Hn1fzzYbCRE.txt\n",
      " Tokenizado: 1358_burGCF_JzDc.txt\n",
      " Tokenizado: 1359_ETB5vX984ig.txt\n",
      " Tokenizado: 1360_QQL8pFn5Hrw.txt\n",
      " Tokenizado: 1361_K4wiilceeGc.txt\n",
      " Tokenizado: 1362_6x-ozPjqLKA.txt\n",
      " Tokenizado: 1363_UC0OTZoYW7M.txt\n",
      " Tokenizado: 1364_8FZnQntY28E.txt\n",
      " Tokenizado: 1365_KLQgta4-CSg.txt\n",
      " Tokenizado: 1366_3cX66R_6x3Y.txt\n",
      " Tokenizado: 1367_9SNXFHl64J0.txt\n",
      " Tokenizado: 1368_Qr3WdkZKGuY.txt\n",
      " Tokenizado: 1369_mZ_54Fc4_JA.txt\n",
      " Tokenizado: 1370_JyaGzJKlXIk.txt\n",
      " Tokenizado: 1371_nR2H66TzSGo.txt\n",
      " Tokenizado: 1372_CmZbcK8nvK0.txt\n",
      " Tokenizado: 1373_5YDG49R3z74.txt\n",
      " Tokenizado: 1374_RUD8ea5dBSE.txt\n",
      " Tokenización del Bloque_13 completada.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "# Carga el modelo de spaCy para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "bloque_actual = 13\n",
    "carpeta_entrada = fr\"D:\\Transcripciones_Mañaneras_Limpias\\Bloque_{bloque_actual}\"\n",
    "carpeta_salida = fr\"D:\\Transcripciones_Mañaneras_Tokenizados\\Bloque_{bloque_actual}\"\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "# Procesa cada archivo .txt del bloque\n",
    "for archivo in os.listdir(carpeta_entrada):\n",
    "    if archivo.endswith(\".txt\"):\n",
    "        try:\n",
    "            ruta_entrada = os.path.join(carpeta_entrada, archivo)\n",
    "            ruta_salida = os.path.join(carpeta_salida, archivo)\n",
    "            \n",
    "            with open(ruta_entrada, \"r\", encoding=\"utf-8\") as f:\n",
    "                texto = f.read()\n",
    "            \n",
    "            # Procesa el texto con spaCy\n",
    "            doc = nlp(texto)\n",
    "            # Extrae los tokens (omite espacios)\n",
    "            tokens = [token.text for token in doc if not token.is_space]\n",
    "            \n",
    "            with open(ruta_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "                for token in tokens:\n",
    "                    f.write(token + \"\\n\")\n",
    "            \n",
    "            print(f\" Tokenizado: {archivo}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error con {archivo}: {e}\")\n",
    "\n",
    "print(f\" Tokenización del Bloque_{bloque_actual} completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d97b45-7f9e-461d-b6a7-d169e84293db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
